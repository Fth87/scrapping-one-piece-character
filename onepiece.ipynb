{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# One Piece Character Episodes Scraper\n",
    "\n",
    "This notebook scrapes character appearance data from One Piece episodes using the One Piece Fandom Wiki.\n",
    "\n",
    "## Overview\n",
    "- **Source**: One Piece Fandom Wiki\n",
    "- **Target**: Character appearances per episode\n",
    "- **Output**: CSV file with character-episode mapping\n",
    "- **Current Episode Range**: 1 - 1141+ episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for web scraping, data processing, and file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63d93064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c25a72",
   "metadata": {},
   "source": [
    "## 2. Configuration and Initialization\n",
    "\n",
    "Set up the main variables and configuration for the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a58d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_episodes = defaultdict(list)\n",
    "\n",
    "# Scraping configuration\n",
    "base_url = \"https://onepiece.fandom.com/wiki/Episode_\"\n",
    "max_episodes = 1141  # Total available episodes (update as needed)\n",
    "\n",
    "# HTTP headers to avoid being blocked\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e832fd0",
   "metadata": {},
   "source": [
    "## 3. Character Extraction Function\n",
    "\n",
    "Define the main function to extract characters from individual episode pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59cfe4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Character extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "def get_characters_from_episode(episode_number):\n",
    "    \"\"\"\n",
    "    Extract characters from a specific One Piece episode page.\n",
    "    \n",
    "    Args:\n",
    "        episode_number (int): The episode number to scrape\n",
    "    \n",
    "    Returns:\n",
    "        list: List of character names that appear in the episode\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}{episode_number}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "        span = soup.find(\"span\", id=\"Characters_in_Order_of_Appearance\")\n",
    "        if not span:\n",
    "            print(f\"No 'Characters in Order of Appearance' section found for Episode {episode_number}\")\n",
    "            return []\n",
    "            \n",
    "        character_section = span.find_parent(\"h2\")\n",
    "        if not character_section:\n",
    "            print(f\"No character section found for Episode {episode_number}\")\n",
    "            return []\n",
    "        \n",
    "        character_list = character_section.find_next('ul')\n",
    "        if not character_list:\n",
    "            print(f\"No character list found for Episode {episode_number}\")\n",
    "            return []\n",
    "\n",
    "        # Extract character names\n",
    "        characters = []\n",
    "        for li in character_list.find_all('li'):\n",
    "            character_name = li.find('a')\n",
    "            \n",
    "            # Try to get the character name from the link title\n",
    "            if character_name and character_name.get('title'):\n",
    "                name = character_name.get('title')\n",
    "            else:\n",
    "                # Fallback to plain text\n",
    "                name = li.get_text(strip=True)\n",
    "            \n",
    "            # Clean the name (remove annotations like \"(flashback)\", \"(debut)\", etc.)\n",
    "            if name:\n",
    "                cleaned_name = name.split('(')[0].strip()\n",
    "                if cleaned_name:  # Only add non-empty names\n",
    "                    characters.append(cleaned_name)\n",
    "\n",
    "        return characters\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network error fetching Episode {episode_number}: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for Episode {episode_number}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Character extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471620c1",
   "metadata": {},
   "source": [
    "## 4. Web Scraping Process\n",
    "\n",
    "Run the main scraping loop to collect character data from all episodes.\n",
    "\n",
    "**⚠️ Note**: For demonstration purposes, we'll start with episodes 1-10. To scrape all episodes, change `max_episodes` back to 1141 or your desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85be5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape episodes 1 to 10...\n",
      "Progress:\n",
      "Scraping Episode 1... Found 8 characters\n",
      "Scraping Episode 2... Found 12 characters\n",
      "Scraping Episode 3... Found 12 characters\n",
      "Scraping Episode 4... Found 18 characters\n",
      "Scraping Episode 5... Found 8 characters\n",
      "Scraping Episode 6... Found 9 characters\n",
      "Scraping Episode 7... Found 14 characters\n",
      "Scraping Episode 8... Found 13 characters\n",
      "Scraping Episode 9... Found 13 characters\n",
      "Scraping Episode 10... Found 12 characters\n",
      "\n",
      "Scraping completed!\n",
      "Total time: 42.14 seconds\n",
      " Successful scrapes: 10\n",
      "Failed scrapes: 0\n",
      "Unique characters found: 50\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, limit to first 10 episodes\n",
    "# Remove this line to scrape all episodes (will take much longer)\n",
    "demo_max_episodes = 10\n",
    "\n",
    "print(f\"Starting to scrape episodes 1 to {demo_max_episodes}...\")\n",
    "print(f\"Progress:\")\n",
    "\n",
    "start_time = time.time()\n",
    "successful_scrapes = 0\n",
    "failed_scrapes = 0\n",
    "\n",
    "for episode in range(1, demo_max_episodes + 1):\n",
    "    print(f\"Scraping Episode {episode}... \", end=\"\")\n",
    "    \n",
    "    characters = get_characters_from_episode(episode)\n",
    "    \n",
    "    if characters:\n",
    "        # Add characters to our data structure\n",
    "        for character in characters:\n",
    "            if character:  # Skip empty character names\n",
    "                character_episodes[character].append(episode)\n",
    "        \n",
    "        print(f\"Found {len(characters)} characters\")\n",
    "        successful_scrapes += 1\n",
    "    else:\n",
    "        print(f\"No characters found\")\n",
    "        failed_scrapes += 1\n",
    "\n",
    "    # Rate limiting: be respectful to the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Summary\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print(f\" Successful scrapes: {successful_scrapes}\")\n",
    "print(f\"Failed scrapes: {failed_scrapes}\")\n",
    "print(f\"Unique characters found: {len(character_episodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f860a",
   "metadata": {},
   "source": [
    "## 5. Data Processing\n",
    "\n",
    "Clean and organize the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "703cc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing character data...\n",
      "\n",
      "Data Summary:\n",
      "Total characters: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing character data...\")\n",
    "\n",
    "# Sort episodes for each character\n",
    "for character in character_episodes:\n",
    "    character_episodes[character].sort()\n",
    "\n",
    "# Display summary statistics\n",
    "total_characters = len(character_episodes)\n",
    "\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"Total characters: {total_characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ab1b4",
   "metadata": {},
   "source": [
    "## 6. Export to CSV\n",
    "\n",
    "Save the processed data to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7802e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output filename\n",
    "csv_file = \"onepiece_characters.csv\"\n",
    "try:\n",
    "    with open(csv_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writerow([\"Character\", \"Episodes\", \"Total_Episodes\"])\n",
    "        \n",
    "        # Write character data (sorted alphabetically)\n",
    "        for character, episodes in sorted(character_episodes.items()):\n",
    "            episode_list = ','.join(map(str, episodes))\n",
    "            total_eps = len(episodes)\n",
    "\n",
    "            writer.writerow([character, episode_list, total_eps])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_summary",
   "metadata": {},
   "source": [
    "## 7. Project Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "This notebook successfully:\n",
    "- Scraped character data from One Piece Fandom Wiki\n",
    "- Extracted character appearances from episode pages\n",
    "- Cleaned and processed the character names\n",
    "- Generated comprehensive statistics\n",
    "- Exported data to CSV format\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
